{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TalkingData (Kaggle)\n",
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_PATH = r\"C:\\Users\\reio\\.kaggle\\competitions\"\n",
    "\n",
    "def load_data(data_path=DATA_PATH):\n",
    "    # PATHS TO FILE\n",
    "    competition = \"talkingdata-adtracking-fraud-detection\"\n",
    "    comp_path = os.path.join(data_path, competition)\n",
    "    train_sample = os.path.join(comp_path, \"train_sample.csv\")\n",
    "    train_path = os.path.join(comp_path, \"train.csv\")\n",
    "    test_path = os.path.join(comp_path, \"test.csv\")\n",
    "    #ssize = 100000\n",
    "    #return pd.read_csv(train_path,nrows=ssize), pd.read_csv(test_path)\n",
    "    return pd.read_csv(train_path), pd.read_csv(test_path)\n",
    "\n",
    "train, test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training sample\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Describe train\n",
    "train.dtypes\n",
    "train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check NAs\n",
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract data where is_attributed == 1\n",
    "train_att = train[train['is_attributed']==1]\n",
    "# Check NAs\n",
    "train_att.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that all the missing values in 'attributed_time' are for observations that did not convert into a download ('is_attributed'=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Percentage of is_attributed == 1\n",
    "p = len(train_att)/len(train)\n",
    "print(len(train_att))\n",
    "print('The percentage of converted clicks is {num:.10%}'.format(num=p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the proportion of clicks that converted into a download or not\n",
    "plt.figure(figsize=(6,6))\n",
    "#sns.set(font_scale=1.2)\n",
    "mean = (train.is_attributed.values == 1).mean()\n",
    "ax = sns.barplot(['Converted (1)', 'Not Converted (0)'], [mean, 1-mean])\n",
    "ax.set(ylabel='Proportion', title='Proportion of clicks converted into app downloads')\n",
    "for p, uniq in zip(ax.patches, [mean, 1-mean]):\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+p.get_width()/2.,\n",
    "            height+0.01,\n",
    "            '{}%'.format(round(uniq * 100, 2)),\n",
    "            ha=\"center\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Undersampling\n",
    "Sample the data using random undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Separate the 2 classes\n",
    "train_0 = train[train['is_attributed'] == 0]\n",
    "train_1 = train[train['is_attributed'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(train_1))\n",
    "print(train_0.shape)\n",
    "print(train.shape)\n",
    "train['is_attributed'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Undersample class 0 (without replacement)\n",
    "train0_undersampled = resample(train_0, replace=False, n_samples=len(train_1), random_state=142) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combine minority class with downsampled majority class\n",
    "train_us = pd.concat([train0_undersampled, train_1])\n",
    " \n",
    "# Display new class counts\n",
    "train_us.is_attributed.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set categorical variables\n",
    "cat = ['ip', 'app', 'device', 'os', 'channel']\n",
    "for c in cat:\n",
    "    train_us[c] = train_us[c].astype('category')\n",
    "    test[c]=test[c].astype('category')\n",
    "\n",
    "# Only training data has is_attributed\n",
    "train_us['is_attributed'] = train_us['is_attribute'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract features from click_time\n",
    "def ppClicktime(df):\n",
    "    df['click_time'] = pd.to_datetime(df['click_time'])\n",
    "    df['wday'] = df['click_time'].dt.dayofweek\n",
    "    df['week'] = df['click_time'].dt.week\n",
    "    df['hour'] = df['click_time'].dt.hour\n",
    "    df['minute'] = df['click_time'].dt.minute\n",
    "    return df\n",
    "# Pre-process training (undersampled) and testing sets\n",
    "train_pp = ppClicktime(train_us)\n",
    "test_pp = ppClicktime(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop click_time\n",
    "train_pp.drop('click_time', axis = 1, inplace = True)\n",
    "test_pp.drop('click_time', axis = 1, inplace = True)\n",
    "print(len(test_pp))\n",
    "test_pp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write to csv\n",
    "train_pp.to_csv(\"train_pp.csv\",index=None)\n",
    "test_pp.to_csv(\"test_pp.csv\",index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load in pre-processed files\n",
    "PP_PATH = r\"C:\\Users\\reio\\Documents\\GitHub\\TalkingData\"\n",
    "\n",
    "def load_pp(pp_path=PP_PATH):\n",
    "    # PATHS TO FILE\n",
    "    train_pp = os.path.join(pp_path, \"train_pp.csv\")\n",
    "    test_pp = os.path.join(pp_path, \"test_pp.csv\")\n",
    "    return pd.read_csv(train_pp), pd.read_csv(test_pp)\n",
    "\n",
    "train_pp, test_pp = load_pp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop attributed_time\n",
    "train_pp.drop('attributed_time', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def aggregate_features(df):\n",
    "    # IPs\n",
    "    n_ip = df[['ip','channel']].groupby(by=['ip'])[['channel']].count().reset_index().rename(index = str, columns={'channel': 'n_ip'})\n",
    "    df = df.merge(n_ip, on = ['ip'], how = 'left')\n",
    "    # wday + hour\n",
    "    ip_wday_hour = df[['ip', 'wday', 'hour', 'channel']].groupby(by = ['ip','wday','hour'])[['channel']].count().reset_index().rename(index = str, columns = {'channel': 'ip_wday_hour'})\n",
    "    df = df.merge(ip_wday_hour, on = ['ip', 'wday', 'hour'], how = 'left')\n",
    "    # app + hour\n",
    "    ip_app_hour = df[['ip', 'app', 'hour', 'channel']].groupby(by = ['ip','app','hour'])[['channel']].count().reset_index().rename(index = str, columns = {'channel': 'ip_app_hour'})\n",
    "    df = df.merge(ip_app_hour, on = ['ip', 'app', 'hour'], how = 'left')\n",
    "    # device + hour\n",
    "    #ip_device_hour = df[['ip', 'device', 'hour', 'channel']].groupby(by = ['ip','device','hour'])[['channel']].count().reset_index().rename(index = str, columns = {'channel': 'ip_device_hour'})\n",
    "    #df = df.merge(ip_device_hour, on = ['ip', 'device', 'hour'], how = 'left')\n",
    "    # app count\n",
    "    #ip_app_count = df[['ip','app', 'channel']].groupby(by=['ip', 'app'])[['channel']].count().reset_index().rename(columns={'channel': 'ip_app_count'})\n",
    "    #df = df.merge(ip_app_count, on = ['ip', 'app'], how = 'left')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_ag = aggregate_features(train_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_ag = aggregate_features(test_pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write to csv\n",
    "train_ag.to_csv(\"train_ag.csv\",index=None)\n",
    "test_ag.to_csv(\"test_ag.csv\",index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load in aggregated files\n",
    "PP_PATH = r\"C:\\Users\\reio\\Documents\\GitHub\\TalkingData\"\n",
    "\n",
    "def load_ag(ag_path=PP_PATH):\n",
    "    # PATHS TO FILE\n",
    "    train_pp = os.path.join(pp_path, \"train_ag.csv\")\n",
    "    test_pp = os.path.join(pp_path, \"test_ag.csv\")\n",
    "    return pd.read_csv(train_ag), pd.read_csv(test_ag)\n",
    "\n",
    "train_ag, test_ag = load_ag()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Separate response variables from predictors\n",
    "y = list(train_ag.is_attributed)\n",
    "X = train_ag.drop(['is_attributed'],axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Drop ip\n",
    "X = X.drop(['ip'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split the training data into training and test sets for cross-validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit model\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "# Predict on test set\n",
    "y_pred = logreg.predict(X_test)\n",
    "y_pred_prob = logreg.predict_proba(X_test)\n",
    "# AUC\n",
    "metrics.roc_auc_score(y_test, y_pred_prob[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit model\n",
    "Ntree = 500\n",
    "rfc = RandomForestClassifier(n_estimators=Ntree)\n",
    "rfc.fit(X_train, y_train)\n",
    "# Predict on test set\n",
    "y_pred = rfc.predict(X_test)\n",
    "y_pred_prob = rfc.predict_proba(X_test)\n",
    "# AUC\n",
    "metrics.roc_auc_score(y_test, y_pred_prob[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target = 'is_attributed'\n",
    "predictors = ['device', 'app', 'os', 'channel', \n",
    "              'hour', 'n_ip', 'ip_wday_hour', 'ip_app_hour']\n",
    "categorical = ['app', 'device', 'os', 'channel', 'hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 255,  \n",
    "    'max_depth': 8,  \n",
    "    'min_child_samples': 100,  \n",
    "    'max_bin': 100,  \n",
    "    'subsample': 0.7,  \n",
    "    'subsample_freq': 1,  \n",
    "    'colsample_bytree': 0.7,  \n",
    "    'min_child_weight': 0,  \n",
    "    'subsample_for_bin': 200000,  \n",
    "    'min_split_gain': 0,  \n",
    "    'reg_alpha': 0,  \n",
    "    'reg_lambda': 0,  \n",
    "   # 'nthread': 8,\n",
    "    'verbose': 0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dtrain = lgb.Dataset(X_train[predictors].values, label=y_train,\n",
    "                      feature_name=predictors,\n",
    "                      categorical_feature=categorical\n",
    "                      )\n",
    "dvalid = lgb.Dataset(X_test.values, label=y_test,\n",
    "                      feature_name=predictors,\n",
    "                      categorical_feature=categorical\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttrain's auc: 0.958592\tvalid's auc: 0.908455\n",
      "Training until validation scores don't improve for 30 rounds.\n",
      "[2]\ttrain's auc: 0.967311\tvalid's auc: 0.913362\n",
      "[3]\ttrain's auc: 0.966109\tvalid's auc: 0.908945\n",
      "[4]\ttrain's auc: 0.966494\tvalid's auc: 0.922239\n",
      "[5]\ttrain's auc: 0.968294\tvalid's auc: 0.922826\n",
      "[6]\ttrain's auc: 0.9694\tvalid's auc: 0.923806\n",
      "[7]\ttrain's auc: 0.969168\tvalid's auc: 0.914936\n",
      "[8]\ttrain's auc: 0.96992\tvalid's auc: 0.914579\n",
      "[9]\ttrain's auc: 0.969724\tvalid's auc: 0.919285\n",
      "[10]\ttrain's auc: 0.969607\tvalid's auc: 0.931189\n",
      "[11]\ttrain's auc: 0.970172\tvalid's auc: 0.931884\n",
      "[12]\ttrain's auc: 0.970504\tvalid's auc: 0.932024\n",
      "[13]\ttrain's auc: 0.970511\tvalid's auc: 0.93739\n",
      "[14]\ttrain's auc: 0.970501\tvalid's auc: 0.939673\n",
      "[15]\ttrain's auc: 0.970821\tvalid's auc: 0.934884\n",
      "[16]\ttrain's auc: 0.971124\tvalid's auc: 0.935265\n",
      "[17]\ttrain's auc: 0.971198\tvalid's auc: 0.934758\n",
      "[18]\ttrain's auc: 0.97144\tvalid's auc: 0.934711\n",
      "[19]\ttrain's auc: 0.971705\tvalid's auc: 0.935667\n",
      "[20]\ttrain's auc: 0.971768\tvalid's auc: 0.934659\n",
      "[21]\ttrain's auc: 0.971971\tvalid's auc: 0.934314\n",
      "[22]\ttrain's auc: 0.972105\tvalid's auc: 0.933102\n",
      "[23]\ttrain's auc: 0.972141\tvalid's auc: 0.933926\n",
      "[24]\ttrain's auc: 0.972373\tvalid's auc: 0.93413\n",
      "[25]\ttrain's auc: 0.972454\tvalid's auc: 0.936254\n",
      "[26]\ttrain's auc: 0.972645\tvalid's auc: 0.93612\n",
      "[27]\ttrain's auc: 0.972874\tvalid's auc: 0.935661\n",
      "[28]\ttrain's auc: 0.972943\tvalid's auc: 0.937122\n",
      "[29]\ttrain's auc: 0.97308\tvalid's auc: 0.936986\n",
      "[30]\ttrain's auc: 0.973184\tvalid's auc: 0.93636\n",
      "[31]\ttrain's auc: 0.973331\tvalid's auc: 0.935943\n",
      "[32]\ttrain's auc: 0.973426\tvalid's auc: 0.935808\n",
      "[33]\ttrain's auc: 0.973544\tvalid's auc: 0.935211\n",
      "[34]\ttrain's auc: 0.973661\tvalid's auc: 0.936362\n",
      "[35]\ttrain's auc: 0.973774\tvalid's auc: 0.936051\n",
      "[36]\ttrain's auc: 0.973902\tvalid's auc: 0.935606\n",
      "[37]\ttrain's auc: 0.974039\tvalid's auc: 0.935078\n",
      "[38]\ttrain's auc: 0.974132\tvalid's auc: 0.934189\n",
      "[39]\ttrain's auc: 0.974176\tvalid's auc: 0.93261\n",
      "[40]\ttrain's auc: 0.974269\tvalid's auc: 0.932493\n",
      "[41]\ttrain's auc: 0.974335\tvalid's auc: 0.931237\n",
      "[42]\ttrain's auc: 0.97441\tvalid's auc: 0.929835\n",
      "[43]\ttrain's auc: 0.974519\tvalid's auc: 0.92888\n",
      "[44]\ttrain's auc: 0.97458\tvalid's auc: 0.928102\n",
      "Early stopping, best iteration is:\n",
      "[14]\ttrain's auc: 0.970501\tvalid's auc: 0.939673\n"
     ]
    }
   ],
   "source": [
    "evals_results = {}\n",
    "lgb_model = lgb.train(params, \n",
    "                 dtrain, \n",
    "                 valid_sets=[dtrain, dvalid], \n",
    "                 valid_names=['train','valid'], \n",
    "                 evals_result=evals_results, \n",
    "                 num_boost_round=350,\n",
    "                 early_stopping_rounds=30,\n",
    "                 verbose_eval=True, \n",
    "                 feval=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict on test dataset and write out submission file\n",
    "test2 = test_ag.drop(['click_id','ip'],axis=1)\n",
    "rfc.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_submit = rfc.predict(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_ag['is_attributed'] = y_submit\n",
    "ans = test_ag[['click_id', 'is_attributed']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ans.to_csv('submission1.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18790469, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
